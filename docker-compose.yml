version: '3.8'

services:
  init-ollama:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_healthy
    command: >
      sh -c "echo 'Scaricamento del modello llama3.2:3b...' &&
             curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"llama3.2:3b\"}' &&
             echo 'Download completato.'"
      

  frontend:
    build: ./frontend
    ports:
      - "${FRONTEND_PORT}:${FRONTEND_PORT}"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - NODE_ENV=${NODE_ENV}
      - FRONTEND_PORT=${FRONTEND_PORT}
      - API_GATEWAY_URL=${API_GATEWAY_URL}
    #restart: always

  stt-service:
    build: ./stt-service
    ports:
      - "${STT_SERVICE_PORT}:${STT_SERVICE_PORT}"
    volumes:
      - ./stt-service:/app
    environment:
      - PYTHONUNBUFFERED=${PYTHONUNBUFFERED}
      - STT_SERVICE_PORT=${STT_SERVICE_PORT}

  emotion-predictor:
    build: ./emotion-predictor
    ports:
      - "${EMOTION_PREDICTOR_PORT}:${EMOTION_PREDICTOR_PORT}"
    volumes:
      - ./emotion-predictor:/app
    environment:
      - PYTHONUNBUFFERED=${PYTHONUNBUFFERED}
      - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
      - EMOTION_PREDICTOR_PORT=${EMOTION_PREDICTOR_PORT}
      - MODEL_PATH=${MODEL_PATH}
      - SCALER_PATH=${SCALER_PATH}
    restart: always

  tts-service:
    build: ./tts-service
    ports:
      - "${TTS_SERVICE_PORT}:${TTS_SERVICE_PORT}"
    volumes:
      - ./tts-service:/app
    environment:
      - PYTHONUNBUFFERED=${PYTHONUNBUFFERED}
      - TTS_SERVICE_PORT=${TTS_SERVICE_PORT}
    #restart: always

  chatbot-service:
    build: ./chatbot-service
    ports:
      - "${CHATBOT_SERVICE_PORT}:${CHATBOT_SERVICE_PORT}"
    volumes:
      - ./chatbot-service:/app
    environment:
      - PYTHONUNBUFFERED=${PYTHONUNBUFFERED}
      - CHATBOT_SERVICE_PORT=${CHATBOT_SERVICE_PORT}
      - CHESHIRE_CAT_URL=${CHESHIRE_CAT_URL}
      - OLLAMA_URL=${OLLAMA_URL}
    depends_on:
      - ollama
      - cheshire-cat-core
    restart: always

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - ${OLLAMA_PORT}:${OLLAMA_PORT}
    volumes:
      - ${OLLAMA_MODELS_PATH}:/root/.ollama
    restart: always
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:11434/api/health || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5

  cheshire-cat-core:
    image: ghcr.io/cheshire-cat-ai/core:latest
    container_name: cheshire_cat_core
    ports:
      - "${CHESHIRE_CAT_PORT_EXTERNAL}:${CHESHIRE_CAT_PORT_INTERNAL}"
      - "5678:5678"
    volumes:
      - ${CHESHIRE_STATIC_PATH}:/app/cat/static
      - ${CHESHIRE_PLUGINS_PATH}:/app/cat/plugins
      - ${CHESHIRE_DATA_PATH}:/app/cat/data
    environment:
      - MODEL_API=${MODEL_API}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - CORE_LLM_SERVICE=${CORE_LLM_SERVICE}
      - OLLAMA_BASE_URL=${OLLAMA_URL}
      - AUTOMATIC_CORE_LLM_LOADING=${AUTOMATIC_CORE_LLM_LOADING}
    depends_on:
      ollama:
        condition: service_healthy
    restart: always

  api-gateway:
    build: ./api-gateway
    ports:
      - "${API_GATEWAY_PORT}:${API_GATEWAY_PORT}"
    volumes:
      #- ./api-gateway/nginx.conf.template:/etc/nginx/nginx.conf.template
      - ./api-gateway/nginx.conf:/etc/nginx/nginx.conf
    environment:
      - WORKER_CONNECTIONS=${WORKER_CONNECTIONS}
      - API_GATEWAY_PORT=${API_GATEWAY_PORT}
      - FRONTEND_PORT=${FRONTEND_PORT}
      - STT_SERVICE_PORT=${STT_SERVICE_PORT}
      - EMOTION_PREDICTOR_PORT=${EMOTION_PREDICTOR_PORT}
      - CHATBOT_SERVICE_PORT=${CHATBOT_SERVICE_PORT}
      - TTS_SERVICE_PORT=${TTS_SERVICE_PORT}
      - CHESHIRE_CAT_PORT_INTERNAL=${CHESHIRE_CAT_PORT_INTERNAL}
    depends_on:
      - frontend
      - stt-service
      - emotion-predictor
      - chatbot-service
      - tts-service
      - cheshire-cat-core
    restart: always

networks:
  default:
    driver: bridge